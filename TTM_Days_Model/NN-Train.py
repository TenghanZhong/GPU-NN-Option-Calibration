# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ general_nn_trainer.py (with testâ€‘set plotting) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import argparse, pathlib, joblib, math
import torch, torch.nn as nn, torch.optim as optim
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
# ---------- 1.  CLI ----------

def get_args():
    p = argparse.ArgumentParser("Generic paramâ†’output NN trainer")
    p.add_argument("--file",      required=True)
    p.add_argument("--in-cols",   required=True, nargs="+")
    p.add_argument("--out-cols",  required=True, nargs="+")
    p.add_argument("--test-size", type=float, default=0.15)
    p.add_argument("--hidden",    type=int,   default=30)   # å• int â†’ å‡åŒ€å±‚
    p.add_argument("--layers",    type=int,   default=3)
    p.add_argument("--act",       default="elu")            # éšè—å±‚æ¿€æ´»
    p.add_argument("--out-act",   default="linear")         # è¾“å‡ºå±‚æ¿€æ´»
    p.add_argument("--epochs",    type=int,   default=200)
    p.add_argument("--batch",     type=int,   default=32)
    p.add_argument("--lr",        type=float, default=1e-3)
    p.add_argument("--device",    default="cpu")
    return p.parse_args()

# ---------- 2.  Data ----------

def load_table(path: pathlib.Path):
    if path.suffix == ".parquet":
        return pd.read_parquet(path)
    if path.suffix == ".csv":
        return pd.read_csv(path)
    raise ValueError("file must be .parquet or .csv")


def split_and_scale(df, in_cols, out_cols, test_size):
    # å…ˆæ„é€ åˆ†å±‚æ ‡ç­¾ï¼št_Kç»„åˆ
    if "t" not in df.columns or "K" not in df.columns:
        raise ValueError("åˆ†å±‚é‡‡æ ·éœ€è¦ df ä¸­åŒ…å« 't' å’Œ 'K' åˆ—")

    strata = df["t"].astype(str) + "_" + df["K"].astype(str)

    # æ‰§è¡Œåˆ†å±‚åˆ’åˆ†
    train_df, test_df = train_test_split(
        df, test_size=test_size, stratify=strata, random_state=2025
    )

    # æ‹†å‡ºè¾“å…¥è¾“å‡º
    X_tr = train_df[in_cols].values
    Y_tr = train_df[out_cols].values
    X_te = test_df[in_cols].values
    Y_te = test_df[out_cols].values

    # æ ‡å‡†åŒ–
    scX, scY = StandardScaler(), StandardScaler()
    X_tr = scX.fit_transform(X_tr);
    X_te = scX.transform(X_te)
    Y_tr = scY.fit_transform(Y_tr);
    Y_te = scY.transform(Y_te)

    return map(torch.tensor, (X_tr, X_te, Y_tr, Y_te)), (scX, scY)


# ---------- 3.  Model ----------

def get_act(name):
    name = name.lower()
    if name in ("linear", "none"):
        return nn.Identity()
    if name == "relu":
        return nn.ReLU()
    if name == "elu":
        return nn.ELU()
    if name == "leaky":
        return nn.LeakyReLU(0.01)
    if name == "tanh":
        return nn.Tanh()
    raise ValueError(f"unknown act {name}")


class GenericNet(nn.Module):
    def __init__(self, dim_in, dim_out, hidden, layers, act="elu", out_act="linear"):
        super().__init__()
        if isinstance(hidden, int):
            hidden = [hidden] * layers
        blocks, in_d = [], dim_in
        for h in hidden:
            blocks += [nn.Linear(in_d, h), get_act(act)]
            in_d = h
        blocks += [nn.Linear(in_d, dim_out), get_act(out_act)]
        self.net = nn.Sequential(*blocks)

    def forward(self, x):
        return self.net(x)


# ---------- 4.  Train ----------

def train(net, opt, Xtr, Ytr, Xte, Yte, epochs, batch, device):
    mse, N = nn.MSELoss(), len(Xtr)
    rmse = lambda a, b: torch.sqrt(mse(a, b))
    for ep in range(1, epochs + 1):
        net.train(); idx = torch.randperm(N); run = 0.0
        for i in range(0, N, batch):
            sl = idx[i : i + batch]
            x, y = Xtr[sl].to(device), Ytr[sl].to(device)
            opt.zero_grad(); loss = rmse(net(x), y); loss.backward(); opt.step()
            run += loss.item()
        net.eval()
        with torch.no_grad():
            val = rmse(net(Xte.to(device)), Yte.to(device)).item()
        print(f"Ep{ep:3d}/{epochs}  train {run / (N // batch):.5f}  val {val:.5f}")


def train_early_stopping(net, opt, Xtr, Ytr, Xte, Yte, epochs, batch, device, patience=20):
    mse, N = nn.MSELoss(), len(Xtr)
    rmse = lambda a, b: torch.sqrt(mse(a, b))

    # åŠ  ReduceLROnPlateau å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        opt, mode='min', factor=0.5, patience=3, verbose=True, min_lr=1e-6
    )

    best_val = float("inf")
    best_weights = None
    best_epoch = 0
    wait = 0
    train_curve = []
    val_curve = []

    for ep in range(1, epochs + 1):
        net.train()
        idx = torch.randperm(N)
        run = 0.0
        for i in range(0, N, batch):
            sl = idx[i: i + batch]
            x, y = Xtr[sl].to(device), Ytr[sl].to(device)
            opt.zero_grad()
            loss = rmse(net(x), y)
            loss.backward()
            opt.step()
            run += loss.item()

        avg_train_rmse = run / (N // batch)
        train_curve.append(avg_train_rmse)

        net.eval()
        with torch.no_grad():
            val_rmse = rmse(net(Xte.to(device)), Yte.to(device)).item()
        val_curve.append(val_rmse)

        # æ‰“å°æœ¬è½®ä¿¡æ¯ + å½“å‰å­¦ä¹ ç‡
        current_lr = opt.param_groups[0]['lr']
        print(f"Ep{ep:3d}/{epochs}  train {avg_train_rmse:.5f}  val {val_rmse:.5f}  lr {current_lr:.2e}")

        # è°ƒæ•´å­¦ä¹ ç‡
        scheduler.step(val_rmse)

        # Early Stopping
        if val_rmse < best_val:
            best_val = val_rmse
            best_epoch = ep
            best_weights = net.state_dict()
            wait = 0
        else:
            wait += 1
            if wait >= patience:
                print(f"\nâ¹ï¸ Early stopping triggered at epoch {ep}.")
                print(f"âœ… Best validation RMSE = {best_val:.5f} at epoch {best_epoch}.")
                break

    # â”€â”€ æ¢å¤æœ€ä½³æƒé‡ â”€â”€
    if best_weights is not None:
        net.load_state_dict(best_weights)
        print(f"ğŸ“¦ Restored best model weights from epoch {best_epoch}.")

    # â”€â”€ ä¿å­˜æœ€ç»ˆç”»å›¾ â”€â”€
    out_dir = pathlib.Path(r"C:\Users\26876\Desktop\Math548\Project")
    out_dir.mkdir(exist_ok=True)

    plt.figure(figsize=(10, 5))
    plt.plot(train_curve, label="Train RMSE", marker="o")
    plt.plot(val_curve, label="Validation RMSE", marker="x")
    plt.scatter(best_epoch - 1, best_val, color="red", s=80, label="Best Epoch")
    plt.xlabel("Epoch")
    plt.ylabel("RMSE")
    plt.title("Training and Validation RMSE")
    plt.grid(True)
    plt.legend()
    plt.tight_layout()
    plt.savefig(out_dir / "train_val_rmse_curve.png", dpi=150)
    plt.show()

    return train_curve, val_curve


# ---------- 5.  Plot ----------

def plot_results(y_true, y_pred, out_dir: pathlib.Path, title="NN prediction vs. true on testâ€‘set"):
    """Scatter/line plot comparing y_true and y_pred for each output dimension."""
    y_true, y_pred = map(lambda t: t.reshape(-1, t.shape[-1]), (y_true, y_pred))
    n_dims = y_true.shape[1]

    plt.figure(figsize=(10, 4 * n_dims))
    for d in range(n_dims):
        ax = plt.subplot(n_dims, 1, d + 1)
        ax.plot(y_true[:, d], label=f"True dim {d}", marker="o", linestyle="", alpha=0.7)
        ax.plot(y_pred[:, d], label=f"Pred dim {d}", marker="x", linestyle="", alpha=0.7)
        ax.set_title(f"Dimension {d}")
        ax.legend()
        ax.grid(True)
    plt.suptitle(title)
    plt.tight_layout(rect=[0, 0, 1, 0.96])
    fname = out_dir / "test_pred_vs_true_5000.png"
    plt.savefig(fname, dpi=150)
    plt.show(block=False)
    print(f"ğŸ“ˆ plot saved to {fname}")
import numpy as np

def plot_results_small_sample(y_true, y_pred, out_dir: pathlib.Path, n_samples=100, title="NN Prediction vs True (Small Sample)"):
    """éšæœºé€‰ n_samples ä¸ªç‚¹ï¼Œç”»é¢„æµ‹ vs çœŸå€¼çš„å¯¹æ¯”å›¾ã€‚"""
    assert y_true.shape == y_pred.shape
    n_total = y_true.shape[0]
    idx = np.random.choice(n_total, size=min(n_samples, n_total), replace=False)

    y_true_sample = y_true[idx]
    y_pred_sample = y_pred[idx]

    plt.figure(figsize=(8, 6))
    plt.scatter(range(len(y_true_sample)), y_true_sample, label="True", marker="o", alpha=0.7)
    plt.scatter(range(len(y_pred_sample)), y_pred_sample, label="Predicted", marker="x", alpha=0.7)
    plt.title(title)
    plt.xlabel("Sample Index")
    plt.ylabel("Put Price")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()

    fname = out_dir / "test_pred_vs_true_small_sample_5000.png"
    plt.savefig(fname, dpi=150)
    plt.show(block=False)
    print(f"ğŸ“ˆ Small sample plot saved to {fname}")

# ---------- 6.  Main ----------

def main(cfg):
    df = load_table(pathlib.Path(cfg["file"]))
    (Xt, Xv, Yt, Yv), (scX, scY) = split_and_scale(
        df, cfg["in_cols"], cfg["out_cols"], cfg["test_size"]
    )
    Xt, Xv, Yt, Yv = [t.double() for t in (Xt, Xv, Yt, Yv)]
    dev = torch.device(cfg["device"])
    net = GenericNet(
        len(cfg["in_cols"]),
        len(cfg["out_cols"]),
        cfg["hidden"],
        cfg["layers"],
        cfg["act"],
        cfg["out_act"],
    ).double().to(dev)
    opt = optim.Adam(net.parameters(), lr=cfg["lr"])
    train_curve, val_curve = train_early_stopping(net, opt, Xt, Yt, Xv, Yv, cfg["epochs"], cfg["batch"], dev, patience=25)

    # â”€â”€ ä¿å­˜è·¯å¾„ â”€â”€
    out = pathlib.Path(r"C:\Users\26876\Desktop\Math548\Project")
    out.mkdir(exist_ok=True)

    # ä¿å­˜æ¨¡å‹æƒé‡
    torch.save(net.state_dict(), out / "model_5000_ratio.pt")

    # ä¿å­˜æ ‡å‡†åŒ–å™¨åŠåˆ—ä¿¡æ¯
    joblib.dump(
        {
            "scX": scX,
            "scY": scY,
            "in_cols": cfg["in_cols"],
            "out_cols": cfg["out_cols"],
        },
        out / "scalers_5000_ratio.pkl",
    )

    # â”€â”€ é¢„æµ‹ & åæ ‡å‡†åŒ– â”€â”€
    net.eval()
    with torch.no_grad():
        preds = net(Xv.to(dev)).cpu().numpy()
    Y_true = Yv.cpu().numpy()

    Y_true_orig = scY.inverse_transform(Y_true)
    preds_orig = scY.inverse_transform(preds)

    # â”€â”€ ç”»å›¾ â”€â”€
    plot_results(Y_true_orig, preds_orig, out)

    print("âœ… Model & plots saved to C:/Users/26876/Desktop/Math548/Project/")

    # â”€â”€ åŸæœ¬ç”»å…¨éƒ¨ç‚¹
    plot_results(Y_true_orig, preds_orig, out)

    # ğŸ†• æ–°åŠ ä¸€è¡Œ â€”â€” åªç”»å°æ ·æœ¬
    plot_results_small_sample(Y_true_orig, preds_orig, out, n_samples=100)

    # â”€â”€ è®¡ç®—æœ€ç»ˆ test set RMSE â”€â”€
    final_test_rmse = math.sqrt(((Y_true_orig - preds_orig) ** 2).mean())
    print(f"ğŸ Final Test Set RMSE (after inverse transform): {final_test_rmse:.6f}")

# ---------- 7.  Run ----------

if __name__ == "__main__":
    # â€”â€”ç”¨ CLI æ—¶æ³¨é‡Šæ‰ä¸‹é¢ cfg å¹¶è°ƒç”¨ main(get_args())â€”â€”
    cfg = dict(
        file=r"C:\Users\26876\Desktop\Math548\Project\Option_put_price_dataset_mpmath_5000.parquet",
        in_cols=["a", "b", "c", "d", "kappa", "r", "t", "K"],
        out_cols=["price_put"],
        test_size=0.05,
        hidden=[64, 32, 32],
        layers=3,
        act="elu",
        out_act="linear",
        epochs=150,
        batch=64,
        lr=2e-3,
        device="cuda",
        dropout=0,
    )
    main(cfg)  # â† ç›´æ¥è¿è¡Œ
    # main(get_args())        # â† æ”¹æˆè¿™è¡Œå³å¯èµ°å‘½ä»¤è¡Œ
'''
 ä» parquet æ–‡ä»¶åŠ è½½ Î¸, t, K â†’ price_put
   â†’ æ•°æ®å½’ä¸€åŒ–
   â†’ æ„å»º NN æ¨¡å‹ï¼šf(Î¸, t, K) â†’ price_put
   â†’ MSE è®­ç»ƒå¹¶ä¿å­˜æ¨¡å‹
   â†’ å¯è§†åŒ– NN åœ¨ test set ä¸Šçš„é¢„æµ‹èƒ½åŠ›
'''