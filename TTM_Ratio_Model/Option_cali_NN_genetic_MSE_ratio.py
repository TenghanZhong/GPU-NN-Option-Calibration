# â””â””â”” æ”¹è¿›ç‰ˆ Option_cali_NN_genetic_2stage.py â””â””â””
"""
2-Stage Calibration:
1. Genetic Algorithm (GA) for global search.
2. Local Optimization (Nelder-Mead) for refinement.
Optimized for better precision and convergence.
Compatible with PyGAD 3.x.x
"""

import pathlib, joblib, numpy as np, pandas as pd, torch
import torch.nn as nn
import pygad, multiprocessing as mp, sys
import matplotlib.pyplot as plt
from scipy.optimize import minimize
import time
import matplotlib.cm as cm

# â€”â€” 1. Paths & Constants â€”â€”
PROJECT_DIR = pathlib.Path(r"C:\Users\26876\Desktop\Math548\Project\TTM_Ratio_Results")
MODEL_W     = PROJECT_DIR / "model_5000_ratio.pt"
SCALERS_PKL = PROJECT_DIR / "scalers_5000_ratio.pkl"
T_LIST = np.array([258, 100, 48, 20], dtype=float)/252

history = []

def record_history(ga_instance):
    global history
    best_fitness = -ga_instance.best_solution()[1]
    history.append(best_fitness)
    print(f"Generation {ga_instance.generations_completed:4d} | Best MSE: {best_fitness:.6e}")

# â€”â€” 2. NN Definition â€”â€”
def get_act(name: str):
    name = name.lower()
    if name in ("linear", "none"): return nn.Identity()
    if name == "relu": return nn.ReLU()
    if name == "elu": return nn.ELU()
    if name == "leaky": return nn.LeakyReLU(0.01)
    if name == "tanh": return nn.Tanh()
    raise ValueError(f"unknown activation: {name}")

class GenericNet(nn.Module):
    def __init__(self, dim_in, dim_out, hidden, layers, act="elu", out_act="linear"):
        super().__init__()
        hidden = [hidden] * layers if isinstance(hidden, int) else hidden
        blocks, in_d = [], dim_in
        for h in hidden:
            blocks += [nn.Linear(in_d, h), get_act(act)]
            in_d = h
        blocks += [nn.Linear(in_d, dim_out), get_act(out_act)]
        self.net = nn.Sequential(*blocks)

    def forward(self, x):
        return self.net(x)

# â€”â€” 3. Helpers â€”â€”
# â”€â”€ 3. Helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_nn(device="cpu"):
    saved = joblib.load(SCALERS_PKL)
    scX, scY, in_cols = saved["scX"], saved["scY"], saved["in_cols"]
    net = (GenericNet(len(in_cols), 1, [64, 32, 32], 3, "elu", "linear")
           .double().to(device))
    net.load_state_dict(torch.load(MODEL_W, map_location=device))
    net.eval()
    return net, scX, scY, in_cols                 # â† æŠŠ in_cols ä¹Ÿè¿”å›

# ---------- ä¿®æ”¹è¿™ä¸€æ®µ ----------
def make_inputs(df: pd.DataFrame, theta: np.ndarray, *, in_cols):
    a, b, c, d, kappa, *rates = theta

    nearest_idx = np.abs(df["T"].values[:, None] - T_LIST[None, :]).argmin(axis=1)
    r_vec = np.take(rates, nearest_idx)

    tmp = df.copy()
    tmp["a"], tmp["b"], tmp["c"], tmp["d"], tmp["kappa"] = a, b, c, d, kappa
    tmp["r"] = r_vec

    # ğŸ†• æ˜ å°„åˆ—åï¼Œç¡®ä¿å’Œè®­ç»ƒæ—¶ä¸€è‡´
    tmp = tmp.rename(columns={"T": "t", "k": "K"})

    X = tmp[in_cols].values
    return X.astype(np.float64)



# â€”â€” 4. Fitness Function â€”â€”
def fitness_factory(df, net, scX, scY, in_cols, device):
    def mse(theta):
        X = make_inputs(df, theta, in_cols=in_cols)  # âœ… ä¿®å¤è¿™é‡Œ
        Xn = torch.tensor(scX.transform(X)).double().to(device)
        with torch.no_grad():
            preds = net(Xn).cpu().numpy()
        preds_orig = scY.inverse_transform(preds)
        err = preds_orig.ravel() - df["opt_price"].values
        return float(np.mean(err**2))

    def fitness_func(ga_instance, solution, solution_idx):
        return -mse(solution)

    return fitness_func


# â€”â€” 5. Calibration Entry â€”â€”
def calibrate(df: pd.DataFrame, device="cpu",
              pop_size=600, num_generations=5000, seed=2024):
    start_time = time.time()

    net, scX, scY, in_cols = load_nn(device)
    fitness_func = fitness_factory(df, net, scX, scY, in_cols, device)

    lows  = np.array([0.0, 0.0, 1e-4, 0.50, 0.0, -0.25, -0.25, -0.25, -0.25])
    highs = np.array([3.0, 5.0, 0.9999, 0.999, 10.0, 0.25, 0.25, 0.25, 0.25])
    bounds = list(zip(lows, highs))
    gene_space = [{'low': lo, 'high': hi} for lo, hi in bounds]
    n_cores = max(1, mp.cpu_count()-2)

    history.clear()

    ga = pygad.GA(
        num_generations=num_generations,
        num_parents_mating=int(pop_size * 0.5),
        fitness_func=fitness_func,
        sol_per_pop=pop_size,
        num_genes=len(gene_space),
        gene_space=gene_space,
        parent_selection_type="rank",#ä¿å®ˆè®¾ç½®sss steady state selection
        crossover_type="two_points",
        mutation_type="adaptive",
        mutation_percent_genes=[40, 15],
        parallel_processing=["thread", n_cores],
        random_seed=seed,
        on_generation=record_history
    )
    ga.run()

    ga_solution, ga_fitness, _ = ga.best_solution()

    print("\nğŸ” GA finished.")
    print(f"GA best MSE: {-ga_fitness:.6e}")

    def mse(theta):
        X = make_inputs(df, theta, in_cols=in_cols)
        Xn = torch.tensor(scX.transform(X)).double().to(device)
        with torch.no_grad():
            preds = net(Xn).cpu().numpy()
        preds_orig = scY.inverse_transform(preds)
        err = preds_orig.ravel() - df["opt_price"].values
        return float(np.mean(err**2))

    result = minimize(
        mse,
        ga_solution,
        method='L-BFGS-B',# or Nelder-Mead
        bounds=bounds,
        options={'maxiter': 3000, 'xatol': 1e-8, 'fatol': 1e-8, 'disp': True}
    )

    solution = result.x
    fitness  = result.fun

    keys = ["a","b","c","d","kappa","r1","r2","r3","r4"]
    params = {k: round(v, 6) for k, v in zip(keys, solution)}
    print("\nâœ… Final optimisation finished.")
    print("Final MSE =", fitness)
    print("ğŸ“Œ Calibrated parameters:")
    for k,v in params.items():
        print(f"  {k:5s}= {v}")

    with open(PROJECT_DIR / "best_params.txt", "w") as f:
        for k,v in params.items():
            f.write(f"{k}={v}\n")

    plt.figure(figsize=(8,4))
    plt.plot(history, marker='o')
    plt.title("GA Calibration Progress")
    plt.xlabel("Generation")
    plt.ylabel("Best MSE")
    plt.grid(True)
    plt.tight_layout()
    plt.savefig(PROJECT_DIR / "progress.png")
    plt.close()
    X_final = make_inputs(df, solution, in_cols=in_cols)
    Xn_final = torch.tensor(scX.transform(X_final)).double().to(device)
    with torch.no_grad():
        preds_final_std = net(Xn_final).cpu().numpy()
    preds_final = scY.inverse_transform(preds_final_std).ravel()

    true_prices = df["opt_price"].values

    # == ç”»æ•£ç‚¹ & å‚è€ƒçº¿ ==
    plt.figure(figsize=(7, 6))

    # â‘  å¸‚åœºä»· â†’ æ¨¡å‹ä»·   ï¼ˆè“è‰² Ã—ï¼‰
    plt.scatter(true_prices, preds_final,
                marker="x", s=60, linewidths=1.5, color="#1f77b4",
                label="Model (Predicted)")

    # â‘¡ å¸‚åœºä»· â†’ å¸‚åœºä»·   ï¼ˆç°è‰² â—‹ï¼Œç©ºå¿ƒæ›´é†’ç›®ï¼‰
    plt.scatter(true_prices, true_prices,
                marker="o", s=40, facecolors="none", edgecolors="gray",
                alpha=0.8, label="Market (Reference)")

    plt.xlabel("Market Prices")
    plt.ylabel("Model Prices")
    plt.title("Market vs Model Prices After Calibration")
    plt.grid(True, linestyle="--", alpha=0.5)
    plt.legend()
    plt.tight_layout()
    plt.show()

    # == ä»¥ Strike ä¸ºæ¨ªåæ ‡ï¼ŒæŒ‰ T åˆ†ç±»ä¸Šè‰²ï¼Œç”» Market vs Model Price ==
    plt.figure(figsize=(7, 5))

    unique_T = np.sort(df["T"].unique())
    cmap_market = cm.get_cmap('Oranges')
    cmap_model = cm.get_cmap('Blues')

    # å¸‚åœºä»·ç”¨æ©™è‰²ç³»åˆ—ï¼Œæ¨¡å‹ä»·ç”¨è“è‰²ç³»åˆ—
    for i, t in enumerate(unique_T):
        mask = df["T"] == t
        label_T = f"T = {t:.3f}y"

        color_market = cmap_market(0.4 + 0.6 * i / max(1, len(unique_T) - 1))
        color_model = cmap_model(0.4 + 0.6 * i / max(1, len(unique_T) - 1))

        # Market price: â—‹ ç©ºå¿ƒ
        plt.scatter(df.loc[mask, "k"], df.loc[mask, "opt_price"],
                    marker='o', facecolors='none', edgecolors=color_market,
                    label=f"Market {label_T}")

        # Model price: + åŠ å·
        plt.scatter(df.loc[mask, "k"], preds_final[mask.values],
                    marker='+', color=color_model, label=f"Model {label_T}")

    plt.xlabel("Strike Price (k)")
    plt.ylabel("Put Option Price")
    plt.title("Put Price vs Strike (by Maturity, Colored)")
    plt.grid(True, linestyle="--", alpha=0.5)
    plt.legend(fontsize=7, ncol=2)
    plt.tight_layout()
    plt.savefig(PROJECT_DIR / "price_vs_strike_by_T_colored.png", dpi=150)
    plt.show()

    print("ğŸ“ˆ Price vs Strike plot saved to 'price_vs_strike_by_T_colored.png'.")

    elapsed = time.time() - start_time  # â±ï¸ ç»“æŸè®¡æ—¶
    print(f"â±ï¸ Total calibration time: {elapsed:.2f} seconds")

    return params, ga, result

# â€”â€” 6. Script Entry â€”â€”
if __name__ == "__main__":
    df = (pd.read_excel(r"C:\Users\26876\Desktop\Math548\Project\02_jan_put.xlsx")
            .rename(columns={"Strike_Put":"k",
                             "TTM_Put":"T",
                             "mid_price_Put":"opt_price"}))
    df["opt_price"] /= 100
    df["T"] /= 365
    calibrate(df, device="cuda")
